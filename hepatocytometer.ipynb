{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch torchvision pillow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T07:40:00.869459Z","iopub.execute_input":"2025-10-14T07:40:00.870036Z","iopub.status.idle":"2025-10-14T07:41:15.447377Z","shell.execute_reply.started":"2025-10-14T07:40:00.870010Z","shell.execute_reply":"2025-10-14T07:41:15.446444Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.19.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.9.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nHemocytometer DETECT + COUNT (모든 라벨 -> 'cell'), NO argparse, FOLDER 입력.\n- 입력: Roboflow TensorFlow export를 이미 풀어둔 폴더\n        (예) DATASET/\n             ├─ train/\n             │   ├─ _annotations.csv\n             │   └─ *.jpg|png|tif...\n             └─ valid/\n                 ├─ _annotations.csv\n                 └─ *.jpg|png|tif...\n- 모델: torchvision SSDLite320 MobileNetV3 (경량) 단일 클래스 학습\n- 출력: OUT_DIR/\n        ├─ models/best.pt\n        ├─ logs.txt\n        ├─ viz_val/*.png   (박스 + gt/pred 총계)\n        ├─ viz_test/*.png\n        ├─ report_val.csv  (image, gt_count, pred_count)\n        └─ report_test.csv\n\"\"\"\n\n# =======================\n# CONFIG (필요시 수정)\n# =======================\nCONFIG = {\n    \"IMAGE_SIZE\": 640,\n    \"BATCH\": 8,\n    \"EPOCHS\": 30,\n    \"LR\": 3e-4,\n    \"WD\": 5e-4,\n    \"CONF_THRESH\": 0.25,\n    \"IOU_MATCH\": 0.50,\n    \"USE_CPU\": False,\n    \"SEED\": 1337,\n    \"VAL_TEST_SPLIT\": 0.5,   # valid를 val/test 반반\n}\n\n# =======================\n# Imports\n# =======================\nimport os, io, csv, random, math, time\nfrom pathlib import Path\nfrom collections import defaultdict\n\nimport numpy as np\nfrom PIL import Image, ImageDraw, ImageFont\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport torchvision\nfrom torchvision.transforms.functional import to_tensor\nfrom torchvision.ops import nms, box_iou\n\nfrom torchvision.models.detection.ssdlite import SSDLiteClassificationHead\n\n# =======================\n# Utils\n# =======================\ndef set_seed(s=1337):\n    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n\ndef ensure_dir(p):\n    Path(p).mkdir(parents=True, exist_ok=True)\n\ndef log(msg, fp=None):\n    print(msg, flush=True)\n    if fp: fp.write(msg + \"\\n\"); fp.flush()\n\ndef load_font():\n    try:\n        return ImageFont.truetype(\"arial.ttf\", 18)\n    except:\n        try:\n            return ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 18)\n        except:\n            return ImageFont.load_default()\n\ndef _csv_value(row, *keys, default=None):\n    for k in keys:\n        if k in row and row[k] != \"\":\n            return row[k]\n    return default\n\n\n# =======================\n# Data Loading (from DIR)\n# =======================\ndef read_split_from_dir(base_dir, split):  # split in {\"train\",\"valid\"}\n    \"\"\"\n    base_dir/train/_annotations.csv, base_dir/valid/_annotations.csv\n    리턴: items = [{path: 이미지 절대경로, W,H, boxes:[[x1,y1,x2,y2],...], labels:[1,...]}]\n    \"\"\"\n    base_dir = Path(base_dir)\n    csv_path = base_dir / split / \"_annotations.csv\"\n    if not csv_path.exists():\n        raise FileNotFoundError(f\"{csv_path} not found\")\n\n    by_file = defaultdict(list)\n    with open(csv_path, \"r\", encoding=\"utf-8\") as f:\n        rows = csv.DictReader(f)\n        for r in rows:\n            fn = _csv_value(r, \"filename\", \"file\", \"image\")\n            if not fn:\n                continue\n            W = int(float(_csv_value(r, \"width\", default=\"0\")))\n            H = int(float(_csv_value(r, \"height\", default=\"0\")))\n            xmin = float(_csv_value(r, \"xmin\", default=\"0\"))\n            ymin = float(_csv_value(r, \"ymin\", default=\"0\"))\n            xmax = float(_csv_value(r, \"xmax\", default=\"0\"))\n            ymax = float(_csv_value(r, \"ymax\", default=\"0\"))\n            by_file[fn].append((W, H, xmin, ymin, xmax, ymax))\n\n    items = []\n    for fn, rows_ in by_file.items():\n        # 이미지 실제 경로 찾기 (보통 split/filename 그대로)\n        img_path = base_dir / split / fn\n        if not img_path.exists():\n            # 확장자만 다를 수 있으니 폴더 내에서 suffix search\n            cand = None\n            for p in (base_dir / split).glob(f\"**/{Path(fn).name}\"):\n                cand = p; break\n            if cand is None:\n                # 동일 basename으로 검색\n                bn = Path(fn).stem\n                for p in (base_dir / split).glob(f\"**/{bn}.*\"):\n                    cand = p; break\n            if cand is None:\n                continue\n            img_path = cand\n\n        W = int(rows_[0][0]); H = int(rows_[0][1])\n        boxes = []\n        for (_, _, x1, y1, x2, y2) in rows_:\n            x1 = max(0.0, min(float(x1), W-1))\n            y1 = max(0.0, min(float(y1), H-1))\n            x2 = max(0.0, min(float(x2), W-1))\n            y2 = max(0.0, min(float(y2), H-1))\n            if x2 > x1 and y2 > y1:\n                boxes.append([x1, y1, x2, y2])\n\n        items.append({\n            \"path\": str(img_path.resolve()),\n            \"W\": W, \"H\": H,\n            \"boxes\": boxes,\n            \"labels\": [1]*len(boxes),  # 단일 클래스 'cell'\n        })\n    return items\n\ndef split_valid_into_val_test(valid_items, test_ratio=0.5, seed=1337):\n    rnd = random.Random(seed)\n    idx = list(range(len(valid_items)))\n    rnd.shuffle(idx)\n    k = int(len(idx) * test_ratio)\n    test_idx = set(idx[:k])\n    val, test = [], []\n    for i, it in enumerate(valid_items):\n        (test if i in test_idx else val).append(it)\n    return val, test\n\n\n# =======================\n# Dataset / Collate\n# =======================\nclass DetectDataset(Dataset):\n    def __init__(self, items, img_size=640, train=True, hflip_p=0.5, vflip_p=0.0):\n        self.items = items\n        self.size = img_size\n        self.train = train\n        self.hflip_p = hflip_p\n        self.vflip_p = vflip_p\n\n    def __len__(self): return len(self.items)\n\n    def __getitem__(self, i):\n        it = self.items[i]\n        im = Image.open(it[\"path\"]).convert(\"RGB\")\n        W, H = im.size\n        im = im.resize((self.size, self.size), Image.BILINEAR)\n\n        # 박스 resize\n        sx = self.size / W; sy = self.size / H\n        boxes = [[x1*sx, y1*sy, x2*sx, y2*sy] for x1,y1,x2,y2 in it[\"boxes\"]]\n\n        # 간단 aug\n        if self.train:\n            if random.random() < self.hflip_p:\n                im = im.transpose(Image.FLIP_LEFT_RIGHT)\n                boxes = [[self.size-x2, y1, self.size-x1, y2] for x1,y1,x2,y2 in boxes]\n            if random.random() < self.vflip_p:\n                im = im.transpose(Image.FLIP_TOP_BOTTOM)\n                boxes = [[x1, self.size-y2, x2, self.size-y1] for x1,y1,x2,y2 in boxes]\n\n        img_t = to_tensor(im)  # [0,1], CxHxW\n        target = {}\n        if len(boxes):\n            target[\"boxes\"] = torch.tensor(boxes, dtype=torch.float32)\n            target[\"labels\"] = torch.ones((len(boxes),), dtype=torch.int64)  # all '1'\n        else:\n            target[\"boxes\"] = torch.zeros((0,4), dtype=torch.float32)\n            target[\"labels\"] = torch.zeros((0,), dtype=torch.int64)\n        target[\"image_id\"] = torch.tensor([i])\n        return img_t, target\n\ndef collate_fn(batch):\n    imgs, targets = list(zip(*batch))\n    return list(imgs), list(targets)\n\n\n# =======================\n# Model\n# =======================\ndef build_model(num_classes=2, probe_size=320):\n    \"\"\"\n    num_classes = background 포함(단일 전경 'cell'이면 2).\n    - 일부 버전: backbone.out_channels 없음 → 더미 텐서로 채널 수 추정\n    - 일부 버전: SSDLiteClassificationHead 가 norm_layer 인자를 요구\n    \"\"\"\n    try:\n        from torchvision.models.detection import (\n            ssdlite320_mobilenet_v3_large,\n            SSDLite320_MobileNet_V3_Large_Weights,\n        )\n        m = ssdlite320_mobilenet_v3_large(\n            weights=SSDLite320_MobileNet_V3_Large_Weights.COCO_V1\n        )\n    except Exception:\n        m = torchvision.models.detection.ssdlite320_mobilenet_v3_large(pretrained=True)\n\n    # 1) feature 채널 수 얻기\n    try:\n        in_channels = m.backbone.out_channels  # (일부 버전에서만 제공)\n        if isinstance(in_channels, int):\n            in_channels = [in_channels]  # 안전장치\n    except Exception:\n        m.eval()\n        with torch.no_grad():\n            dummy = torch.zeros(1, 3, probe_size, probe_size)\n            feats = m.backbone(dummy)             # OrderedDict of tensors\n            in_channels = [t.shape[1] for t in feats.values()]\n\n    # 2) 레벨별 anchor 수\n    num_anchors = m.anchor_generator.num_anchors_per_location()  # list[int]\n\n    # 3) 분류 head 교체 (버전 차이 대응)\n    try:\n        # 일부 버전은 norm_layer 필요 없음\n        m.head.classification_head = SSDLiteClassificationHead(\n            in_channels=in_channels,\n            num_anchors=num_anchors,\n            num_classes=num_classes,\n        )\n    except TypeError:\n        # 당신 환경처럼 norm_layer 필수인 버전\n        m.head.classification_head = SSDLiteClassificationHead(\n            in_channels=in_channels,\n            num_anchors=num_anchors,\n            num_classes=num_classes,\n            norm_layer=nn.BatchNorm2d,\n        )\n    return m\n\n\n\n# =======================\n# Evaluation (박스/카운트)\n# =======================\n@torch.no_grad()\ndef simple_eval(model, loader, device, score_thresh=0.25, iou_thresh=0.5, viz_dir=None, names=None):\n    font = load_font()\n    ensure_dir(viz_dir) if viz_dir else None\n\n    tot_tp=tot_fp=tot_fn=0\n    abs_err=[]\n\n    for bidx, (imgs, targets) in enumerate(loader):\n        imgs = [im.to(device) for im in imgs]\n        outs = model(imgs)\n\n        for i, (pred, gt) in enumerate(zip(outs, targets)):\n            boxes = pred[\"boxes\"].detach().cpu()\n            scores = pred[\"scores\"].detach().cpu()\n            keep = scores >= score_thresh\n            boxes = boxes[keep]; scores = scores[keep]\n            if boxes.numel():\n                keep_idx = nms(boxes, scores, 0.5)\n                boxes = boxes[keep_idx]\n\n            gt_boxes = gt[\"boxes\"].cpu()\n\n            # greedy IoU match\n            if len(boxes)>0 and len(gt_boxes)>0:\n                ious = box_iou(boxes, gt_boxes)\n                matched_p=set(); matched_g=set()\n                while True:\n                    v, idx = torch.max(ious, dim=1)\n                    p = int(torch.argmax(v))\n                    g = int(torch.argmax(ious[p]))\n                    if p in matched_p or g in matched_g or ious[p,g] < iou_thresh:\n                        break\n                    matched_p.add(p); matched_g.add(g)\n                    ious[p,:]=0; ious[:,g]=0\n                tp = len(matched_p)\n                fp = len(boxes) - tp\n                fn = len(gt_boxes) - tp\n            else:\n                tp = 0; fp = int(len(boxes)); fn = int(len(gt_boxes))\n\n            tot_tp += tp; tot_fp += fp; tot_fn += fn\n\n            pred_count = int(len(boxes))\n            gt_count = int(len(gt_boxes))\n            abs_err.append(abs(pred_count - gt_count))\n\n            # viz\n            if viz_dir:\n                im = (imgs[i].cpu().numpy().transpose(1,2,0)*255).astype(np.uint8)\n                pil = Image.fromarray(im)\n                draw = ImageDraw.Draw(pil)\n                for b in boxes:\n                    x1,y1,x2,y2 = [float(x) for x in b]\n                    draw.rectangle([x1,y1,x2,y2], outline=(255,0,255), width=2)\n                txt = f\"gt:{gt_count}  pred:{pred_count}\"\n                draw.rectangle([6,6,6+220,6+26], fill=(0,0,0))\n                draw.text((10,8), txt, fill=(255,255,255), font=font)\n                name = names[bidx] if names and bidx < len(names) else f\"img{bidx:06d}\"\n                ensure_dir(viz_dir)\n                pil.save(Path(viz_dir)/f\"viz_{name}_{i:06d}.png\")\n\n    prec = tot_tp / max(1, tot_tp + tot_fp)\n    rec  = tot_tp / max(1, tot_tp + tot_fn)\n    f1   = 2*prec*rec / max(1e-9, (prec+rec))\n    mae  = float(np.mean(abs_err)) if len(abs_err) else float(\"nan\")\n    return {\"precision\":prec, \"recall\":rec, \"f1\":f1, \"mae\":mae}\n\n\n# =======================\n# Train / Test\n# =======================\ndef main():\n    # ---- 입력 받기 (argparse 없음) ----\n    base_dir ='/kaggle/input/hemocytomer'\n    if not base_dir:\n        raise SystemExit(\"Folder path required.\")\n    out_dir = '/kaggle/working/'\n\n    C = CONFIG\n    set_seed(C[\"SEED\"])\n    device = torch.device(\"cpu\" if C[\"USE_CPU\"] or not torch.cuda.is_available() else \"cuda\")\n\n    out_dir = Path(out_dir); ensure_dir(out_dir); ensure_dir(out_dir/\"models\")\n    logf = open(out_dir/\"logs.txt\", \"w\", encoding=\"utf-8\")\n    log(f\"Device: {device}\", logf)\n    log(f\"Base dir: {base_dir}\", logf)\n\n    # 데이터 로드\n    train_items = read_split_from_dir(base_dir, \"train\")\n    valid_items = read_split_from_dir(base_dir, \"valid\")\n    val_items, test_items = split_valid_into_val_test(valid_items, test_ratio=C[\"VAL_TEST_SPLIT\"], seed=C[\"SEED\"])\n    log(f\"Train {len(train_items)}  Val {len(val_items)}  Test {len(test_items)}\", logf)\n\n    ds_tr = DetectDataset(train_items, img_size=C[\"IMAGE_SIZE\"], train=True,  hflip_p=0.5, vflip_p=0.0)\n    ds_va = DetectDataset(val_items,   img_size=C[\"IMAGE_SIZE\"], train=False)\n    ds_te = DetectDataset(test_items,  img_size=C[\"IMAGE_SIZE\"], train=False)\n\n    dl_tr = DataLoader(ds_tr, batch_size=C[\"BATCH\"], shuffle=True,  num_workers=2, collate_fn=collate_fn)\n    dl_va = DataLoader(ds_va, batch_size=C[\"BATCH\"], shuffle=False, num_workers=2, collate_fn=collate_fn)\n    dl_te = DataLoader(ds_te, batch_size=C[\"BATCH\"], shuffle=False, num_workers=2, collate_fn=collate_fn)\n\n    # 모델\n    model = build_model(num_classes=2).to(device)\n    opt = optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=C[\"LR\"], weight_decay=C[\"WD\"])\n\n    best_f1 = -1.0\n    best_file = out_dir/\"models\"/\"best.pt\"\n\n    # ---- 학습 ----\n    for ep in range(1, C[\"EPOCHS\"]+1):\n        model.train()\n        loss_sum = 0.0; n=0\n        t0 = time.time()\n        for imgs, targets in dl_tr:\n            imgs = [im.to(device) for im in imgs]\n            tgts = [{k:(v.to(device) if torch.is_tensor(v) else v) for k,v in t.items()} for t in targets]\n            losses = model(imgs, tgts)      # dict of losses\n            loss = sum(v for v in losses.values())\n            opt.zero_grad(); loss.backward(); opt.step()\n            loss_sum += float(loss.item()); n += 1\n        train_loss = loss_sum / max(1,n)\n        dt = time.time()-t0\n\n        # ---- 검증 ----\n        model.eval()\n        val_metrics = simple_eval(\n            model, dl_va, device,\n            score_thresh=C[\"CONF_THRESH\"], iou_thresh=C[\"IOU_MATCH\"],\n            viz_dir=str(out_dir/\"viz_val\"),\n            names=[Path(it[\"path\"]).name for it in val_items],\n        )\n        log(f\"[{ep:03d}] loss={train_loss:.4f}  val_F1={val_metrics['f1']:.3f}  val_MAE={val_metrics['mae']:.2f}  ({dt:.1f}s)\", logf)\n\n        if val_metrics[\"f1\"] > best_f1:\n            best_f1 = val_metrics[\"f1\"]\n            torch.save({\"epoch\": ep, \"model\": model.state_dict(), \"val\": val_metrics}, best_file)\n            log(f\"  saved best -> {best_file}\", logf)\n\n    # ---- 테스트 ----\n    ckpt = torch.load(best_file, map_location=device)\n    model.load_state_dict(ckpt[\"model\"]); model.eval()\n\n    val_metrics = simple_eval(\n        model, dl_va, device,\n        score_thresh=C[\"CONF_THRESH\"], iou_thresh=C[\"IOU_MATCH\"],\n        viz_dir=str(out_dir/\"viz_val\"),\n        names=[Path(it[\"path\"]).name for it in val_items],\n    )\n    test_metrics = simple_eval(\n        model, dl_te, device,\n        score_thresh=C[\"CONF_THRESH\"], iou_thresh=C[\"IOU_MATCH\"],\n        viz_dir=str(out_dir/\"viz_test\"),\n        names=[Path(it[\"path\"]).name for it in test_items],\n    )\n    log(f\"FINAL  Val: P={val_metrics['precision']:.3f} R={val_metrics['recall']:.3f} F1={val_metrics['f1']:.3f} MAE={val_metrics['mae']:.2f}\", logf)\n    log(f\"FINAL Test: P={test_metrics['precision']:.3f} R={test_metrics['recall']:.3f} F1={test_metrics['f1']:.3f} MAE={test_metrics['mae']:.2f}\", logf)\n\n    # ---- per-image count CSV ----\n    def dump_counts(loader, items, out_csv):\n        ensure_dir(Path(out_csv).parent)\n        with torch.no_grad(), open(out_csv, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n            w = csv.writer(f); w.writerow([\"image\", \"gt_count\", \"pred_count\"])\n            for (imgs, targets), it in zip(loader, items):\n                im = imgs[0].to(device)\n                pred = model([im])[0]\n                boxes = pred[\"boxes\"].detach().cpu()\n                scores = pred[\"scores\"].detach().cpu()\n                keep = scores >= CONFIG[\"CONF_THRESH\"]\n                boxes = boxes[keep]\n                if boxes.numel():\n                    k = nms(boxes, scores[keep], 0.5)\n                    boxes = boxes[k]\n                w.writerow([it[\"path\"], len(it[\"boxes\"]), int(len(boxes))])\n\n    dump_counts(dl_va, val_items,  out_dir/\"report_val.csv\")\n    dump_counts(dl_te, test_items, out_dir/\"report_test.csv\")\n\n    log(\"Done.\", logf); logf.close()\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-14T07:57:35.026024Z","iopub.execute_input":"2025-10-14T07:57:35.026317Z","iopub.status.idle":"2025-10-14T08:09:05.437828Z","shell.execute_reply.started":"2025-10-14T07:57:35.026293Z","shell.execute_reply":"2025-10-14T08:09:05.437073Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nBase dir: /kaggle/input/hemocytomer\nTrain 160  Val 20  Test 20\n[001] loss=7.4368  val_F1=0.034  val_MAE=201.00  (19.8s)\n  saved best -> /kaggle/working/models/best.pt\n[002] loss=4.5756  val_F1=0.048  val_MAE=163.95  (18.3s)\n  saved best -> /kaggle/working/models/best.pt\n[003] loss=3.8666  val_F1=0.063  val_MAE=131.05  (17.8s)\n  saved best -> /kaggle/working/models/best.pt\n[004] loss=3.5358  val_F1=0.098  val_MAE=83.70  (17.6s)\n  saved best -> /kaggle/working/models/best.pt\n[005] loss=3.3631  val_F1=0.370  val_MAE=5.00  (18.3s)\n  saved best -> /kaggle/working/models/best.pt\n[006] loss=3.1754  val_F1=0.282  val_MAE=2.20  (17.5s)\n[007] loss=3.0692  val_F1=0.377  val_MAE=3.10  (18.1s)\n  saved best -> /kaggle/working/models/best.pt\n[008] loss=2.9301  val_F1=0.355  val_MAE=2.40  (18.5s)\n[009] loss=2.8000  val_F1=0.346  val_MAE=2.45  (18.0s)\n[010] loss=2.7231  val_F1=0.379  val_MAE=2.50  (16.5s)\n  saved best -> /kaggle/working/models/best.pt\n[011] loss=2.6075  val_F1=0.387  val_MAE=2.15  (17.8s)\n  saved best -> /kaggle/working/models/best.pt\n[012] loss=2.5561  val_F1=0.377  val_MAE=1.80  (17.7s)\n[013] loss=2.4632  val_F1=0.351  val_MAE=2.15  (17.3s)\n[014] loss=2.4229  val_F1=0.317  val_MAE=2.05  (17.6s)\n[015] loss=2.3520  val_F1=0.350  val_MAE=1.75  (18.4s)\n[016] loss=2.1769  val_F1=0.387  val_MAE=1.70  (17.5s)\n[017] loss=2.1431  val_F1=0.411  val_MAE=1.95  (18.0s)\n  saved best -> /kaggle/working/models/best.pt\n[018] loss=2.0938  val_F1=0.372  val_MAE=2.45  (17.6s)\n[019] loss=2.0734  val_F1=0.342  val_MAE=1.65  (17.8s)\n[020] loss=1.9123  val_F1=0.336  val_MAE=1.90  (16.7s)\n[021] loss=1.8016  val_F1=0.373  val_MAE=2.05  (17.6s)\n[022] loss=1.7867  val_F1=0.395  val_MAE=2.15  (17.3s)\n[023] loss=1.7097  val_F1=0.373  val_MAE=1.60  (18.1s)\n[024] loss=1.6045  val_F1=0.368  val_MAE=2.65  (18.2s)\n[025] loss=1.4799  val_F1=0.394  val_MAE=2.35  (17.4s)\n[026] loss=1.4521  val_F1=0.342  val_MAE=2.05  (17.3s)\n[027] loss=1.3353  val_F1=0.352  val_MAE=2.40  (17.9s)\n[028] loss=1.2366  val_F1=0.378  val_MAE=2.45  (18.2s)\n[029] loss=1.1609  val_F1=0.337  val_MAE=2.55  (18.0s)\n[030] loss=1.0788  val_F1=0.378  val_MAE=2.10  (16.7s)\nFINAL  Val: P=0.446 R=0.380 F1=0.411 MAE=1.95\nFINAL Test: P=0.413 R=0.369 F1=0.390 MAE=1.75\nDone.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nSSDLite-MobileNetV3 (단일 클래스 'cell') 로드 + 임곗값 스윕(Val MAE 최적) + 폴더 추론(+사각형/총계)\n- 학습 시 썼던 헤드 교체 로직(버전 호환) 포함\n- Roboflow TensorFlow export의 valid/_annotations.csv 를 사용해 conf 스윕 가능\n- argparse 없음: 상단 CONFIG or input() 사용\n\"\"\"\n\n# =======================\n# CONFIG (원하면 수정)\n# =======================\nCONFIG = {\n    \"WEIGHTS_PATH\": \"/kaggle/working/models/best.pt\",   # 학습된 가중치\n    \"TUNE_BASE_DIR\": \"\",       # conf 스윕용 데이터 폴더(풀린 상태). 예: \"/kaggle/input/hemocytomer\" (valid/_annotations.csv 필요). 비우면 스킵\n    \"PREDICT_FOLDER\": \"\",      # 추론할 이미지 폴더(비우면 스킵)\n    \"OUT_DIR\": \"/kaggle/working/infer_out\",  # 추론 결과 출력 폴더\n    \"IMAGE_SIZE\": 640,         # 학습 때 썼던 입력 크기와 맞추면 편함\n    \"NMS_IOU\": 0.45,           # 중복 제거 강도\n    \"SIZE_MIN\": 12*12,         # 면적 하한(격자 교차점 등 FP 억제용). None이면 비활성\n    \"SIZE_MAX\": 80*80,         # 면적 상한. None이면 비활성\n    \"CONF_CAND\": [0.05, 0.08, 0.10, 0.12, 0.15, 0.18, 0.20, 0.22, 0.25, 0.30],\n    \"USE_CPU\": False,\n}\n\n# =======================\n# Imports\n# =======================\nimport os, io, csv, random\nfrom pathlib import Path\nfrom collections import defaultdict\n\nimport numpy as np\nfrom PIL import Image, ImageDraw, ImageFont\n\nimport torch\nimport torch.nn as nn\nfrom torchvision.transforms.functional import to_tensor\nfrom torchvision.ops import nms, box_iou\nimport torchvision\nfrom torchvision.models.detection.ssdlite import SSDLiteClassificationHead\n\n# =======================\n# Utils\n# =======================\ndef ensure_dir(p):\n    Path(p).mkdir(parents=True, exist_ok=True)\n\ndef load_font():\n    try:\n        from PIL import ImageFont\n        return ImageFont.truetype(\"arial.ttf\", 18)\n    except:\n        try:\n            return ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 18)\n        except:\n            return ImageFont.load_default()\n\n# =======================\n# Model (버전 호환)\n# =======================\ndef build_model(num_classes=2, probe_size=320):\n    \"\"\"\n    num_classes: background 포함(단일 전경 'cell'이면 2).\n    - out_channels 없을 수 있어 더미 텐서로 채널 수 추정\n    - 일부 버전은 SSDLiteClassificationHead에 norm_layer 필요\n    \"\"\"\n    try:\n        from torchvision.models.detection import (\n            ssdlite320_mobilenet_v3_large,\n            SSDLite320_MobileNet_V3_Large_Weights,\n        )\n        m = ssdlite320_mobilenet_v3_large(\n            weights=SSDLite320_MobileNet_V3_Large_Weights.COCO_V1\n        )\n    except Exception:\n        m = torchvision.models.detection.ssdlite320_mobilenet_v3_large(pretrained=True)\n\n    # in_channels 추정\n    try:\n        in_channels = m.backbone.out_channels\n        if isinstance(in_channels, int):\n            in_channels = [in_channels]\n    except Exception:\n        m.eval()\n        with torch.no_grad():\n            dummy = torch.zeros(1, 3, probe_size, probe_size)\n            feats = m.backbone(dummy)  # OrderedDict[name->Tensor(B,C,H,W)]\n            in_channels = [t.shape[1] for t in feats.values()]\n\n    num_anchors = m.anchor_generator.num_anchors_per_location()\n\n    # classification head 교체\n    try:\n        m.head.classification_head = SSDLiteClassificationHead(\n            in_channels=in_channels,\n            num_anchors=num_anchors,\n            num_classes=num_classes,\n        )\n    except TypeError:\n        m.head.classification_head = SSDLiteClassificationHead(\n            in_channels=in_channels,\n            num_anchors=num_anchors,\n            num_classes=num_classes,\n            norm_layer=nn.BatchNorm2d,\n        )\n    return m\n\ndef load_best_model(weights_path, device=None):\n    device = torch.device(\"cpu\" if (CONFIG[\"USE_CPU\"] or not torch.cuda.is_available()) else \"cuda\") if device is None else device\n    model = build_model(num_classes=2).to(device)\n    ckpt = torch.load(weights_path, map_location=device)\n    model.load_state_dict(ckpt[\"model\"])\n    model.eval()\n    return model, device\n\n# =======================\n# Val 데이터 로더 (Roboflow TF export)\n# =======================\ndef _csv_value(row, *keys, default=None):\n    for k in keys:\n        if k in row and row[k] != \"\":\n            return row[k]\n    return default\n\ndef read_valid_from_dir(base_dir):\n    \"\"\"\n    base_dir/valid/_annotations.csv 를 읽어 (이미지 경로, GT 박스) 목록 반환\n    \"\"\"\n    base = Path(base_dir)\n    csv_path = base / \"valid\" / \"_annotations.csv\"\n    if not csv_path.exists():\n        raise FileNotFoundError(f\"{csv_path} not found\")\n    by_file = defaultdict(list)\n    with open(csv_path, \"r\", encoding=\"utf-8\") as f:\n        rows = csv.DictReader(f)\n        for r in rows:\n            fn = _csv_value(r, \"filename\", \"file\", \"image\")\n            if not fn: \n                continue\n            W = int(float(_csv_value(r, \"width\", default=\"0\")))\n            H = int(float(_csv_value(r, \"height\", default=\"0\")))\n            x1 = float(_csv_value(r, \"xmin\", default=\"0\"))\n            y1 = float(_csv_value(r, \"ymin\", default=\"0\"))\n            x2 = float(_csv_value(r, \"xmax\", default=\"0\"))\n            y2 = float(_csv_value(r, \"ymax\", default=\"0\"))\n            by_file[fn].append((W, H, x1, y1, x2, y2))\n\n    items = []\n    for fn, rows_ in by_file.items():\n        img_path = base / \"valid\" / fn\n        if not img_path.exists():\n            # suffix search\n            cand = None\n            for p in (base/\"valid\").glob(f\"**/{Path(fn).name}\"):\n                cand = p; break\n            if cand is None:\n                bn = Path(fn).stem\n                for p in (base/\"valid\").glob(f\"**/{bn}.*\"):\n                    cand = p; break\n            if cand is None:\n                continue\n            img_path = cand\n\n        W, H = int(rows_[0][0]), int(rows_[0][1])\n        boxes = []\n        for (_, _, a, b, c, d) in rows_:\n            a = max(0.0, min(float(a), W-1))\n            b = max(0.0, min(float(b), H-1))\n            c = max(0.0, min(float(c), W-1))\n            d = max(0.0, min(float(d), H-1))\n            if c > a and d > b:\n                boxes.append([a, b, c, d])\n        items.append({\"path\": str(img_path), \"W\": W, \"H\": H, \"boxes\": boxes})\n    return items\n\n# =======================\n# Inference helpers\n# =======================\n@torch.no_grad()\ndef predict_image(image_path, model, device,\n                  image_size=640, conf=0.15, nms_iou=0.45,\n                  size_min=12*12, size_max=80*80,\n                  draw=False, out_path=None):\n    im = Image.open(image_path).convert(\"RGB\")\n    im_r = im.resize((image_size, image_size), Image.BILINEAR)\n    x = to_tensor(im_r).to(device).unsqueeze(0)\n\n    out = model([x[0]])[0]\n    boxes = out[\"boxes\"].detach().cpu()\n    scores = out[\"scores\"].detach().cpu()\n\n    # score filter + NMS\n    keep = scores >= conf\n    boxes = boxes[keep]; scores = scores[keep]\n    if boxes.numel():\n        k = nms(boxes, scores, nms_iou)\n        boxes = boxes[k]; scores = scores[k]\n\n    # area filter\n    if (size_min is not None) or (size_max is not None):\n        wh = boxes[:, 2:4] - boxes[:, 0:2]\n        area = wh[:, 0] * wh[:, 1]\n        keep = torch.ones(len(area), dtype=torch.bool)\n        if size_min is not None:\n            keep &= (area >= size_min)\n        if size_max is not None:\n            keep &= (area <= size_max)\n        boxes = boxes[keep]; scores = scores[keep]\n\n    count = int(len(boxes))\n\n    if draw:\n        drw = ImageDraw.Draw(im_r)\n        for b in boxes:\n            x1, y1, x2, y2 = [float(z) for z in b]\n            drw.rectangle([x1, y1, x2, y2], outline=(255, 0, 255), width=2)\n        drw.rectangle([6, 6, 220, 32], fill=(0, 0, 0))\n        drw.text((10, 10), f\"count: {count}\", fill=(255, 255, 255), font=load_font())\n        if out_path:\n            ensure_dir(Path(out_path).parent)\n            im_r.save(out_path)\n\n    return {\"count\": count, \"boxes\": boxes}\n\n# =======================\n# CONF 스윕 (Val MAE 최소값)\n# =======================\n@torch.no_grad()\ndef best_conf_by_mae(model, device, valid_items,\n                     image_size=640, nms_iou=0.45, size_min=12*12, size_max=80*80,\n                     conf_cand=None):\n    conf_cand = conf_cand or [0.05, 0.08, 0.10, 0.12, 0.15, 0.18, 0.20, 0.22, 0.25, 0.30]\n    results = []\n    for conf in conf_cand:\n        abs_err = []\n        for it in valid_items:\n            r = predict_image(\n                it[\"path\"], model, device,\n                image_size=image_size, conf=conf, nms_iou=nms_iou,\n                size_min=size_min, size_max=size_max,\n                draw=False\n            )\n            gt = len(it[\"boxes\"])\n            abs_err.append(abs(r[\"count\"] - gt))\n        mae = float(np.mean(abs_err)) if abs_err else float(\"nan\")\n        print(f\"[CONF SWEEP] conf={conf:.2f} -> val_MAE={mae:.3f}\")\n        results.append((conf, mae))\n    # best by MAE\n    results = sorted(results, key=lambda x: x[1])\n    return results[0], results  # (best_conf, full_list)\n\n# =======================\n# 폴더 일괄 추론\n# =======================\n@torch.no_grad()\ndef predict_folder(folder, out_dir, model, device,\n                   image_size=640, conf=0.15, nms_iou=0.45, size_min=12*12, size_max=80*80):\n    out_dir = Path(out_dir); ensure_dir(out_dir); ensure_dir(out_dir/\"viz\")\n    exts = (\".png\",\".jpg\",\".jpeg\",\".tif\",\".tiff\",\".bmp\")\n    paths = [str(p) for p in Path(folder).rglob(\"*\") if p.suffix.lower() in exts]\n\n    with open(out_dir/\"predict_counts.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as f:\n        w = csv.writer(f); w.writerow([\"image\",\"pred_count\"])\n        for p in paths:\n            r = predict_image(\n                p, model, device, image_size=image_size, conf=conf, nms_iou=nms_iou,\n                size_min=size_min, size_max=size_max,\n                draw=True, out_path=out_dir/\"viz\"/(Path(p).stem+\"_viz.png\")\n            )\n            w.writerow([p, r[\"count\"]])\n    print(f\"Saved {out_dir/'predict_counts.csv'} and viz images under {out_dir/'viz'}\")\n\n# =======================\n# Main (input() 기반)\n# =======================\ndef main():\n    # 경로 입력(비우면 CONFIG 사용)\n    wp =  CONFIG[\"WEIGHTS_PATH\"]\n    tune_dir =  CONFIG[\"TUNE_BASE_DIR\"]\n    pred_dir = CONFIG[\"PREDICT_FOLDER\"]\n    out_dir =  CONFIG[\"OUT_DIR\"]\n\n    device = torch.device(\"cpu\" if (CONFIG[\"USE_CPU\"] or not torch.cuda.is_available()) else \"cuda\")\n    print(f\"Device: {device}\")\n\n    # 모델 로드\n    model, device = load_best_model(wp, device=device)\n\n    # 1) conf 스윕 (선택)\n    best_conf = None\n    if tune_dir:\n        valid_items = read_valid_from_dir(tune_dir)\n        (best_conf, best_mae), _all = best_conf_by_mae(\n            model, device, valid_items,\n            image_size=CONFIG[\"IMAGE_SIZE\"], nms_iou=CONFIG[\"NMS_IOU\"],\n            size_min=CONFIG[\"SIZE_MIN\"], size_max=CONFIG[\"SIZE_MAX\"],\n            conf_cand=CONFIG[\"CONF_CAND\"]\n        )\n        print(f\"[BEST] conf={best_conf:.2f}  val_MAE={best_mae:.3f}\")\n\n    # 2) 폴더 추론 (선택)\n    if pred_dir:\n        use_conf = float(best_conf) if best_conf is not None else float(CONFIG[\"CONF_THRESH\"]) if \"CONF_THRESH\" in CONFIG else 0.15\n        print(f\"Predict with conf={use_conf:.2f}\")\n        predict_folder(\n            pred_dir, out_dir, model, device,\n            image_size=CONFIG[\"IMAGE_SIZE\"], conf=use_conf, nms_iou=CONFIG[\"NMS_IOU\"],\n            size_min=CONFIG[\"SIZE_MIN\"], size_max=CONFIG[\"SIZE_MAX\"]\n        )\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T09:00:33.457800Z","iopub.execute_input":"2025-10-14T09:00:33.458634Z","iopub.status.idle":"2025-10-14T09:00:33.841411Z","shell.execute_reply.started":"2025-10-14T09:00:33.458611Z","shell.execute_reply":"2025-10-14T09:00:33.840814Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# === 붙여넣기: 학습된 모델로 폴더 예측 ===\ndef load_best_model(weights_path, num_classes=2):\n    m = build_model(num_classes=num_classes)\n    ckpt = torch.load(weights_path, map_location=\"cpu\")\n    m.load_state_dict(ckpt[\"model\"])\n    m.eval()\n    return m\n\n@torch.no_grad()\ndef predict_on_folder(folder, out_dir, weights_path, image_size=640, conf=0.15, nms_iou=0.5, use_cpu=False):\n    from torchvision.transforms.functional import to_tensor\n    from torchvision.ops import nms\n    import csv\n    device = torch.device(\"cpu\" if use_cpu or not torch.cuda.is_available() else \"cuda\")\n    os.makedirs(out_dir, exist_ok=True)\n    os.makedirs(os.path.join(out_dir, \"viz\"), exist_ok=True)\n\n    model = load_best_model(weights_path, num_classes=2).to(device)\n\n    exts = (\".png\",\".jpg\",\".jpeg\",\".tif\",\".tiff\",\".bmp\")\n    paths = [str(p) for p in Path(folder).rglob(\"*\") if p.suffix.lower() in exts]\n    rep = open(os.path.join(out_dir, \"predict_counts.csv\"), \"w\", encoding=\"utf-8\", newline=\"\")\n    w = csv.writer(rep); w.writerow([\"image\",\"pred_count\"])\n\n    font = load_font()\n\n    for p in paths:\n        im = Image.open(p).convert(\"RGB\")\n        im_r = im.resize((image_size, image_size), Image.BILINEAR)\n        x = to_tensor(im_r).to(device).unsqueeze(0)  # 1xCxHxW\n        out = model([x[0]])[0]\n        boxes = out[\"boxes\"].detach().cpu()\n        scores = out[\"scores\"].detach().cpu()\n        keep = scores >= conf\n        boxes = boxes[keep]; scores = scores[keep]\n        if boxes.numel():\n            k = nms(boxes, scores, nms_iou)\n            boxes = boxes[k]\n        pred_count = int(len(boxes))\n\n        # 시각화\n        draw = ImageDraw.Draw(im_r)\n        for b in boxes:\n            x1,y1,x2,y2 = [float(z) for z in b]\n            draw.rectangle([x1,y1,x2,y2], outline=(255,0,255), width=2)\n        draw.rectangle([6,6,220,32], fill=(0,0,0))\n        draw.text((10,10), f\"count: {pred_count}\", fill=(255,255,255), font=font)\n        im_r.save(os.path.join(out_dir, \"viz\", Path(p).stem+\"_viz.png\"))\n\n        w.writerow([p, pred_count])\n\n    rep.close()\n    print(f\"Saved: {out_dir}/predict_counts.csv and {out_dir}/viz/*.png\")\n\n# 사용 예:\n# predict_on_folder(\"/kaggle/input/my_new_images\", \"/kaggle/working/infer_out\",\n#                   \"/kaggle/working/models/best.pt\", image_size=CONFIG[\"IMAGE_SIZE\"],\n#                   conf=CONFIG[\"CONF_THRESH\"], use_cpu=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}